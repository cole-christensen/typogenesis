#!/usr/bin/env python3
"""
PyTorch Dataset for kerning pairs.

This dataset loads glyph pair images and their associated kerning values
for training kerning prediction models.

Usage:
    from datasets.kerning_dataset import KerningDataset

    dataset = KerningDataset(
        data_dir="./kerning_pairs",
        split="train"
    )

    for pair_image, kerning_value in dataset:
        # pair_image: torch.Tensor [1, H, W*2] (two glyphs side by side)
        # kerning_value: torch.Tensor [1] (normalized kerning value)
        pass
"""

import json
import logging
import random
from pathlib import Path
from typing import Optional, Callable, Union

import numpy as np
import torch
from torch.utils.data import Dataset
from PIL import Image

logger = logging.getLogger(__name__)


class KerningDataset(Dataset):
    """
    PyTorch Dataset for kerning pairs.

    Loads pair images generated by generate_pairs.py and their
    associated kerning values for training KerningNet.

    The kerning values are normalized by units_per_em so they're
    comparable across fonts.
    """

    def __init__(
        self,
        data_dir: Union[str, Path],
        split: str = "train",
        split_ratio: tuple[float, float, float] = (0.8, 0.1, 0.1),
        transform: Optional[Callable] = None,
        normalize_target: bool = True,
        filter_zero_kerning: bool = False,
        balance_dataset: bool = False,
        random_seed: int = 42
    ):
        """
        Initialize the kerning dataset.

        Args:
            data_dir: Directory containing images and metadata subdirs
            split: One of "train", "val", "test", or "all"
            split_ratio: Ratio for train/val/test splits
            transform: Optional transform to apply to images
            normalize_target: Use normalized kerning values (by UPM)
            filter_zero_kerning: Exclude pairs with zero kerning
            balance_dataset: Balance positive/negative kerning samples
            random_seed: Random seed for reproducibility
        """
        self.data_dir = Path(data_dir)
        self.split = split
        self.split_ratio = split_ratio
        self.transform = transform
        self.normalize_target = normalize_target
        self.filter_zero_kerning = filter_zero_kerning
        self.balance_dataset = balance_dataset

        # Find directories
        self.image_dir = self.data_dir / "images"
        self.metadata_dir = self.data_dir / "metadata"

        if not self.image_dir.exists():
            raise ValueError(f"Image directory not found: {self.image_dir}")
        if not self.metadata_dir.exists():
            raise ValueError(f"Metadata directory not found: {self.metadata_dir}")

        # Set random seed
        random.seed(random_seed)
        np.random.seed(random_seed)

        # Load samples
        self.samples = self._load_samples()

        # Apply split
        if split != "all":
            indices = list(range(len(self.samples)))
            random.shuffle(indices)

            n_train = int(len(indices) * split_ratio[0])
            n_val = int(len(indices) * split_ratio[1])

            if split == "train":
                indices = indices[:n_train]
            elif split == "val":
                indices = indices[n_train:n_train + n_val]
            elif split == "test":
                indices = indices[n_train + n_val:]

            self.samples = [self.samples[i] for i in indices]

        logger.info(f"Loaded {len(self.samples)} kerning pairs for {split} split")

    def _load_samples(self) -> list[dict]:
        """Load all valid kerning pair samples."""
        samples = []

        # Find all metadata files
        metadata_files = list(self.metadata_dir.glob("*.json"))

        for metadata_path in metadata_files:
            try:
                with open(metadata_path) as f:
                    data = json.load(f)
            except Exception as e:
                logger.debug(f"Could not load {metadata_path}: {e}")
                continue

            # Get kerning value
            if self.normalize_target:
                kerning = data.get("kerning_normalized", 0.0)
            else:
                kerning = data.get("kerning_value", 0.0)

            # Filter zero kerning if requested
            if self.filter_zero_kerning and kerning == 0:
                continue

            # Check image exists
            image_path = data.get("image_path")
            if image_path:
                full_image_path = self.data_dir / image_path
                if not full_image_path.exists():
                    continue
            else:
                # Try to find image by pair_id
                pair_id = data.get("pair_id", metadata_path.stem)
                full_image_path = self.image_dir / f"{pair_id}.png"
                if not full_image_path.exists():
                    continue
                image_path = f"images/{pair_id}.png"

            sample = {
                "image_path": str(self.data_dir / image_path),
                "kerning_value": data.get("kerning_value", 0.0),
                "kerning_normalized": data.get("kerning_normalized", 0.0),
                "units_per_em": data.get("units_per_em", 1000),
                "left_char": data.get("left_char", ""),
                "right_char": data.get("right_char", ""),
                "left_unicode": data.get("left_unicode", 0),
                "right_unicode": data.get("right_unicode", 0),
                "font_family": data.get("font_family", ""),
                "font_style": data.get("font_style", ""),
                "pair_id": data.get("pair_id", metadata_path.stem)
            }

            samples.append(sample)

        # Balance dataset if requested
        if self.balance_dataset:
            samples = self._balance_samples(samples)

        return samples

    def _balance_samples(self, samples: list[dict]) -> list[dict]:
        """Balance positive and negative kerning samples."""
        positive = [s for s in samples if s["kerning_normalized"] > 0]
        negative = [s for s in samples if s["kerning_normalized"] < 0]
        zero = [s for s in samples if s["kerning_normalized"] == 0]

        # Find the minimum count
        min_count = min(len(positive), len(negative), len(zero) if zero else float('inf'))

        if min_count == float('inf') or min_count == 0:
            return samples

        # Sample equally from each category
        balanced = []
        if positive:
            balanced.extend(random.sample(positive, min(len(positive), min_count)))
        if negative:
            balanced.extend(random.sample(negative, min(len(negative), min_count)))
        if zero and not self.filter_zero_kerning:
            balanced.extend(random.sample(zero, min(len(zero), min_count)))

        random.shuffle(balanced)
        return balanced

    def __len__(self) -> int:
        return len(self.samples)

    def __getitem__(self, idx: int) -> tuple[torch.Tensor, torch.Tensor, dict]:
        """
        Get a kerning pair sample.

        Returns:
            Tuple of (pair_image, kerning_value, metadata)
        """
        sample = self.samples[idx]

        # Load image
        image = Image.open(sample["image_path"]).convert('L')

        # Convert to tensor
        image_array = np.array(image, dtype=np.float32) / 255.0
        image_tensor = torch.from_numpy(image_array).unsqueeze(0)  # [1, H, W]

        # Apply transform if provided
        if self.transform:
            image_tensor = self.transform(image_tensor)

        # Get kerning value
        if self.normalize_target:
            kerning = sample["kerning_normalized"]
        else:
            kerning = sample["kerning_value"]

        kerning_tensor = torch.tensor([kerning], dtype=torch.float32)

        # Metadata
        metadata = {
            "left_char": sample["left_char"],
            "right_char": sample["right_char"],
            "font_family": sample["font_family"],
            "kerning_raw": sample["kerning_value"],
            "kerning_normalized": sample["kerning_normalized"],
            "units_per_em": sample["units_per_em"],
            "pair_id": sample["pair_id"]
        }

        return image_tensor, kerning_tensor, metadata

    def get_statistics(self) -> dict:
        """Get dataset statistics."""
        kerning_values = [s["kerning_normalized"] for s in self.samples]

        stats = {
            "total_samples": len(self.samples),
            "non_zero_samples": sum(1 for k in kerning_values if k != 0),
            "positive_kerning": sum(1 for k in kerning_values if k > 0),
            "negative_kerning": sum(1 for k in kerning_values if k < 0),
            "mean_kerning": np.mean(kerning_values) if kerning_values else 0,
            "std_kerning": np.std(kerning_values) if kerning_values else 0,
            "min_kerning": min(kerning_values) if kerning_values else 0,
            "max_kerning": max(kerning_values) if kerning_values else 0,
            "unique_fonts": len(set(s["font_family"] for s in self.samples)),
            "unique_pairs": len(set((s["left_char"], s["right_char"]) for s in self.samples))
        }

        return stats

    @staticmethod
    def collate_fn(batch):
        """Custom collate function for DataLoader."""
        images = torch.stack([item[0] for item in batch])
        kerning_values = torch.stack([item[1] for item in batch])

        metadata = {
            "left_char": [item[2]["left_char"] for item in batch],
            "right_char": [item[2]["right_char"] for item in batch],
            "font_family": [item[2]["font_family"] for item in batch],
            "kerning_raw": torch.tensor([item[2]["kerning_raw"] for item in batch]),
            "kerning_normalized": torch.tensor([item[2]["kerning_normalized"] for item in batch]),
            "pair_id": [item[2]["pair_id"] for item in batch]
        }

        return images, kerning_values, metadata


class KerningRegressionDataset(KerningDataset):
    """
    Dataset optimized for kerning regression.

    Returns normalized kerning values scaled to a suitable range
    for neural network regression.
    """

    def __init__(
        self,
        *args,
        scale_factor: float = 10.0,
        clip_range: tuple[float, float] = (-1.0, 1.0),
        **kwargs
    ):
        """
        Initialize the regression dataset.

        Args:
            scale_factor: Scale normalized kerning by this factor
            clip_range: Clip scaled values to this range
            *args, **kwargs: Passed to parent KerningDataset
        """
        super().__init__(*args, **kwargs)
        self.scale_factor = scale_factor
        self.clip_range = clip_range

    def __getitem__(self, idx: int) -> tuple[torch.Tensor, torch.Tensor, dict]:
        """Get a sample with scaled kerning value."""
        image_tensor, kerning_tensor, metadata = super().__getitem__(idx)

        # Scale kerning value
        scaled_kerning = kerning_tensor * self.scale_factor

        # Clip to range
        scaled_kerning = torch.clamp(
            scaled_kerning,
            self.clip_range[0],
            self.clip_range[1]
        )

        return image_tensor, scaled_kerning, metadata


class KerningClassificationDataset(KerningDataset):
    """
    Dataset for kerning classification (tight/normal/loose).

    Converts continuous kerning values to discrete classes.
    """

    def __init__(
        self,
        *args,
        num_classes: int = 3,
        thresholds: Optional[list[float]] = None,
        **kwargs
    ):
        """
        Initialize the classification dataset.

        Args:
            num_classes: Number of kerning classes
            thresholds: Threshold values for class boundaries
            *args, **kwargs: Passed to parent KerningDataset
        """
        super().__init__(*args, **kwargs)
        self.num_classes = num_classes

        if thresholds is None:
            # Default thresholds for 3 classes: tight, normal, loose
            # Based on normalized kerning values
            self.thresholds = [-0.02, 0.02]
        else:
            self.thresholds = thresholds

        assert len(self.thresholds) == num_classes - 1

    def _value_to_class(self, kerning: float) -> int:
        """Convert kerning value to class index."""
        for i, threshold in enumerate(self.thresholds):
            if kerning < threshold:
                return i
        return len(self.thresholds)

    def __getitem__(self, idx: int) -> tuple[torch.Tensor, torch.Tensor, dict]:
        """Get a sample with class label."""
        image_tensor, kerning_tensor, metadata = super().__getitem__(idx)

        # Convert to class
        kerning_value = kerning_tensor.item()
        class_idx = self._value_to_class(kerning_value)
        class_tensor = torch.tensor([class_idx], dtype=torch.long)

        # Add class info to metadata
        metadata["kerning_class"] = class_idx
        metadata["class_name"] = self.get_class_name(class_idx)

        return image_tensor, class_tensor, metadata

    def get_class_name(self, class_idx: int) -> str:
        """Get human-readable class name."""
        if self.num_classes == 3:
            names = ["tight", "normal", "loose"]
            return names[class_idx] if class_idx < len(names) else f"class_{class_idx}"
        return f"class_{class_idx}"

    def get_class_distribution(self) -> dict[str, int]:
        """Get distribution of classes in dataset."""
        dist = {}
        for sample in self.samples:
            kerning = sample["kerning_normalized"]
            class_idx = self._value_to_class(kerning)
            class_name = self.get_class_name(class_idx)
            dist[class_name] = dist.get(class_name, 0) + 1
        return dist


def create_kerning_dataloaders(
    data_dir: Union[str, Path],
    batch_size: int = 32,
    num_workers: int = 4,
    task: str = "regression",
    **kwargs
):
    """
    Create train/val/test dataloaders for kerning prediction.

    Args:
        data_dir: Root data directory
        batch_size: Batch size
        num_workers: DataLoader workers
        task: "regression" or "classification"
        **kwargs: Additional arguments for dataset

    Returns:
        Tuple of (train_loader, val_loader, test_loader)
    """
    from torch.utils.data import DataLoader

    if task == "classification":
        DatasetClass = KerningClassificationDataset
    elif task == "regression":
        DatasetClass = KerningRegressionDataset
    else:
        DatasetClass = KerningDataset

    train_dataset = DatasetClass(
        data_dir=data_dir,
        split="train",
        **kwargs
    )

    val_dataset = DatasetClass(
        data_dir=data_dir,
        split="val",
        **kwargs
    )

    test_dataset = DatasetClass(
        data_dir=data_dir,
        split="test",
        **kwargs
    )

    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        collate_fn=KerningDataset.collate_fn,
        pin_memory=True
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        collate_fn=KerningDataset.collate_fn,
        pin_memory=True
    )

    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        collate_fn=KerningDataset.collate_fn,
        pin_memory=True
    )

    return train_loader, val_loader, test_loader


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Test KerningDataset")
    parser.add_argument("--data-dir", type=Path, required=True, help="Data directory")
    parser.add_argument("--split", type=str, default="train", help="Split to load")
    parser.add_argument("--task", type=str, default="regression",
                        choices=["base", "regression", "classification"],
                        help="Task type")
    args = parser.parse_args()

    logging.basicConfig(level=logging.INFO)

    if args.task == "classification":
        dataset = KerningClassificationDataset(
            data_dir=args.data_dir,
            split=args.split
        )
        print("\nClass Distribution:")
        for class_name, count in dataset.get_class_distribution().items():
            print(f"  {class_name}: {count}")
    elif args.task == "regression":
        dataset = KerningRegressionDataset(
            data_dir=args.data_dir,
            split=args.split
        )
    else:
        dataset = KerningDataset(
            data_dir=args.data_dir,
            split=args.split
        )

    print(f"\nKerningDataset Statistics ({args.task}):")
    stats = dataset.get_statistics()
    for key, value in stats.items():
        if isinstance(value, float):
            print(f"  {key}: {value:.6f}")
        else:
            print(f"  {key}: {value}")

    # Test loading a sample
    if len(dataset) > 0:
        image, target, metadata = dataset[0]
        print(f"\nSample 0:")
        print(f"  Image shape: {image.shape}")
        print(f"  Target: {target}")
        print(f"  Left char: {metadata['left_char']}")
        print(f"  Right char: {metadata['right_char']}")
        print(f"  Font: {metadata['font_family']}")
